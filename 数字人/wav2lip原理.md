# wav2lip原理

## 数字人wav2lip模型为啥要用6个通道，其中3个是遮住嘴巴的图？只用三个通道不行吗？
1. 原因：
    - 因为只给一张图片，不知道它在不同的声音驱动下正确的嘴型同步后的图像分别应该是什么样子。
    - 也就是你不知道训练时候的ground truth。
2. 用图解释
   ![](.images/d9b3661d.png)
   - 如果图像输入仅仅有输入2：不对，因为推理需要基本copy输入1的上半部分。
   - 如果图像输入仅仅有输入1：没有mask肯定不行，输入和ground truth相同，那样就不需要训练的。 有mask也不行，鼻子嘴巴模型没见过全靠生成难以训练好。
   - 所以：必须有输入2让模型知道鼻子嘴巴的样子，必须有输入1让模型正确输出上半部分，输入1必须有mask来排除与ground truth相同部分带来的干扰。

3. 思考
   - 如果有已经成熟的模型嘴型同步模型，用他的结果去训练。就可以只需要3通道输入，且训练速度应该快很多。
   - wav训练和推理的输入是不一样的：训练输入两图的上半部分不相同，推理的时候相同。 这可能要求更长的训练时间来解决这种不同。
   - 另一种3通道的训练方案：输入1的上部分+输入2的下半部分合成一个图。 可以试试。没这样做我觉得是合成的图中线过渡不自然+人脸检测误差带来的像素特征丢失。
   - 还有一种3通道的训练方案：仅仅有输入2或者输入1。可以试试，预计训练会很慢。
   - 如果是训练的单人模型：3通道或许可行。

## 关于数据尺寸问题：
1. 音频mel频谱：帧率fps是80，一秒的数据是80x80。换算成25fps就是3.2x80。 音频周期T是5，就是16x80。梅尔倒频谱就是80x16。
2. 音视频统一帧率fps=25。
3. 音频**永远**以周期T=5出现，就是16x80。
4. syncnet网络训练：face=（B,   3xT,h//2, w)，audio=（B,  1,16,80）
5. wav2lip网络训练：face=（B*T, 6,  h   , w)，audio=（B*T,1,80,16）-->(B*T, 3, h , w)，用的倒频谱,音频取了25帧，视频取5帧。
6. wav2lip验证syncnet：把（B*T, 3,  h   , w)变形+裁剪（B,   3xT,h//2, w)
7. wav2lip网络推理：face=（1, 6,  h   , w)，audio=（1,1,80,16）-->(1, 3, h , w)
